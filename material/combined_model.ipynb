{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "combined_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WitTvFuZTz-f",
        "colab_type": "text"
      },
      "source": [
        "copy from https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE3Ke-D0RhIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ec22958c-269a-4742-83f6-47c985d3cac9"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2166\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 13.17 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yRkrnx0Rg-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\t# determine (1) the unique zip codes and (2) the number of data\n",
        "\t# points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\n",
        "\t# loop over each of the unique zip codes and their corresponding\n",
        "\t# count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\n",
        "\t# return the data frame\n",
        "\treturn df\n",
        "\n",
        "def process_house_attributes(df, train, test):\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\n",
        "\t# performin min-max scaling each continuous feature column to\n",
        "\t# the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
        "\ttestContinuous = cs.transform(test[continuous])\n",
        "\n",
        "\t# one-hot encode the zip code categorical data (by definition of\n",
        "\t# one-hot encoing, all output features are now in the range [0, 1])\n",
        "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "\t# construct our training and testing data points by concatenating\n",
        "\t# the categorical features with the continuous features\n",
        "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
        "\ttestX = np.hstack([testCategorical, testContinuous])\n",
        "\n",
        "\t# return the concatenated training and testing data\n",
        "\treturn (trainX, testX)\n",
        "\n",
        "def load_house_images(df, inputPath):\n",
        "\t# initialize our images array (i.e., the house images themselves)\n",
        "\timages = []\n",
        "\n",
        "\t# loop over the indexes of the houses\n",
        "\tfor i in df.index.values:\n",
        "\t\t# find the four images for the house and sort the file paths,\n",
        "\t\t# ensuring the four are always in the *same order*\n",
        "\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "\t\thousePaths = sorted(list(glob.glob(basePath)))\n",
        "\n",
        "\t\t# initialize our list of input images along with the output image\n",
        "\t\t# after *combining* the four input images\n",
        "\t\tinputImages = []\n",
        "\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\n",
        "\t\t# loop over the input house paths\n",
        "\t\tfor housePath in housePaths:\n",
        "\t\t\t# load the input image, resize it to be 32 32, and then\n",
        "\t\t\t# update the list of input images\n",
        "\t\t\timage = cv2.imread(housePath)\n",
        "\t\t\timage = cv2.resize(image, (32, 32))\n",
        "\t\t\tinputImages.append(image)\n",
        "\n",
        "\t\t# tile the four input images in the output image such the first\n",
        "\t\t# image goes in the top-right corner, the second image in the\n",
        "\t\t# top-left corner, the third image in the bottom-right corner,\n",
        "\t\t# and the final image in the bottom-left corner\n",
        "\t\toutputImage[0:32, 0:32] = inputImages[0]\n",
        "\t\toutputImage[0:32, 32:64] = inputImages[1]\n",
        "\t\toutputImage[32:64, 32:64] = inputImages[2]\n",
        "\t\toutputImage[32:64, 0:32] = inputImages[3]\n",
        "\n",
        "\t\t# add the tiled image to our set of images the network will be\n",
        "\t\t# trained on\n",
        "\t\timages.append(outputImage)\n",
        "\n",
        "\t# return our set of images\n",
        "\treturn np.array(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyQFtVoR7XC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb96bc96-a432-4f45-9c20-4eb178a3fa20"
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "def create_mlp(dim, regress=False):\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "\t# return our model\n",
        "\treturn model\n",
        "\n",
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "\t# initialize the input shape and channel dimension, assuming\n",
        "\t# TensorFlow/channels-last ordering\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\n",
        "\t# define the model input\n",
        "\tinputs = Input(shape=inputShape)\n",
        "\n",
        "\t# loop over the number of filters\n",
        "\tfor (i, f) in enumerate(filters):\n",
        "\t\t# if this is the first CONV layer then set the input\n",
        "\t\t# appropriately\n",
        "\t\tif i == 0:\n",
        "\t\t\tx = inputs\n",
        "\n",
        "\t\t# CONV => RELU => BN => POOL\n",
        "\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "\t# flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "\tx = Flatten()(x)\n",
        "\tx = Dense(16)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\tx = Dropout(0.5)(x)\n",
        "\n",
        "\t# apply another FC layer, this one to match the number of nodes\n",
        "\t# coming out of the MLP\n",
        "\tx = Dense(4)(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tx = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "\t# construct the CNN\n",
        "\tmodel = Model(inputs, x)\n",
        "\n",
        "\t# return the CNN\n",
        "\treturn model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4YumZT0XRpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import concatenate\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjW7AT0NXzF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "de6f3622-d226-4ff7-e0ff-e85d17ef04aa"
      },
      "source": [
        "df = load_house_attributes(\"Houses-dataset/Houses Dataset/HousesInfo.txt\")\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2520</td>\n",
              "      <td>93446</td>\n",
              "      <td>789000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1802</td>\n",
              "      <td>93446</td>\n",
              "      <td>365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2146</td>\n",
              "      <td>93446</td>\n",
              "      <td>455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>4</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2464</td>\n",
              "      <td>91901</td>\n",
              "      <td>599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1845</td>\n",
              "      <td>91901</td>\n",
              "      <td>529800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    bedrooms  bathrooms  area  zipcode   price\n",
              "30         5        3.0  2520    93446  789000\n",
              "32         3        2.0  1802    93446  365000\n",
              "39         3        3.0  2146    93446  455000\n",
              "80         4        2.5  2464    91901  599000\n",
              "81         2        2.0  1845    91901  529800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edSeWi6DXnsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "458fba48-2b0b-402d-ec4c-92598b68c4a8"
      },
      "source": [
        "images = load_house_images(df, 'Houses-dataset/Houses Dataset')\n",
        "print(images.shape)\n",
        "plt.imshow(images[0])\n",
        "images = images / 255.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(362, 64, 64, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgPvDEWRYtVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df, images, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qnyjTl3Y1Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6QlmycY4JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainAttrX, testAttrX) = process_house_attributes(df, trainAttrX, testAttrX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNauO02NaVYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c1af70f-f81a-4e0c-863d-f4b1a7a7a3d3"
      },
      "source": [
        "mlp = create_mlp(trainAttrX.shape[1], regress=False)\n",
        "cnn = create_cnn(64, 64, 3, regress=False)\n",
        "\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        "\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "output = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=output)\n",
        "\n",
        "optimizer = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=optimizer)\n",
        "\n",
        "model.fit(\n",
        "\t[trainAttrX, trainImagesX], trainY,\n",
        "\tvalidation_data=([testAttrX, testImagesX], testY),\n",
        "\tepochs=1000, batch_size=512)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 271 samples, validate on 91 samples\n",
            "Epoch 1/1000\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1436.0046 - val_loss: 234.5222\n",
            "Epoch 2/1000\n",
            "271/271 [==============================] - 0s 567us/step - loss: 671.1796 - val_loss: 243.3373\n",
            "Epoch 3/1000\n",
            "271/271 [==============================] - 0s 541us/step - loss: 801.0450 - val_loss: 212.6572\n",
            "Epoch 4/1000\n",
            "271/271 [==============================] - 0s 514us/step - loss: 731.9962 - val_loss: 156.7648\n",
            "Epoch 5/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 573.3498 - val_loss: 135.1743\n",
            "Epoch 6/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 505.4255 - val_loss: 138.8141\n",
            "Epoch 7/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 411.5955 - val_loss: 139.8020\n",
            "Epoch 8/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 313.5435 - val_loss: 138.9979\n",
            "Epoch 9/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 335.1776 - val_loss: 139.4551\n",
            "Epoch 10/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 325.8217 - val_loss: 142.8911\n",
            "Epoch 11/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 345.8900 - val_loss: 150.5354\n",
            "Epoch 12/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 300.5446 - val_loss: 153.3798\n",
            "Epoch 13/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 300.7013 - val_loss: 152.8000\n",
            "Epoch 14/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 282.2661 - val_loss: 152.1133\n",
            "Epoch 15/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 269.1789 - val_loss: 150.7448\n",
            "Epoch 16/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 289.4439 - val_loss: 149.8022\n",
            "Epoch 17/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 256.9503 - val_loss: 149.1194\n",
            "Epoch 18/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 288.8357 - val_loss: 148.4015\n",
            "Epoch 19/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 245.2928 - val_loss: 148.1351\n",
            "Epoch 20/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 230.5662 - val_loss: 146.8374\n",
            "Epoch 21/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 240.3968 - val_loss: 145.3080\n",
            "Epoch 22/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 231.7478 - val_loss: 143.6422\n",
            "Epoch 23/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 225.1761 - val_loss: 141.7991\n",
            "Epoch 24/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 209.3049 - val_loss: 139.7623\n",
            "Epoch 25/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 237.5640 - val_loss: 137.5957\n",
            "Epoch 26/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 167.4852 - val_loss: 135.3350\n",
            "Epoch 27/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 179.7341 - val_loss: 133.0275\n",
            "Epoch 28/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 168.9548 - val_loss: 130.6266\n",
            "Epoch 29/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 152.6334 - val_loss: 128.1018\n",
            "Epoch 30/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 182.4996 - val_loss: 125.5022\n",
            "Epoch 31/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 169.2213 - val_loss: 122.8934\n",
            "Epoch 32/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 146.6307 - val_loss: 120.2833\n",
            "Epoch 33/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 160.8394 - val_loss: 117.6945\n",
            "Epoch 34/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 143.7633 - val_loss: 115.1633\n",
            "Epoch 35/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 128.5007 - val_loss: 112.6568\n",
            "Epoch 36/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 159.7845 - val_loss: 110.1703\n",
            "Epoch 37/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 128.3383 - val_loss: 107.6954\n",
            "Epoch 38/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 139.3440 - val_loss: 105.3407\n",
            "Epoch 39/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 148.8559 - val_loss: 103.1442\n",
            "Epoch 40/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 115.1735 - val_loss: 100.9370\n",
            "Epoch 41/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 120.5827 - val_loss: 98.8043\n",
            "Epoch 42/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 132.7142 - val_loss: 96.7065\n",
            "Epoch 43/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 116.2549 - val_loss: 94.9078\n",
            "Epoch 44/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 109.2929 - val_loss: 93.5287\n",
            "Epoch 45/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 100.0057 - val_loss: 92.5129\n",
            "Epoch 46/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 106.2364 - val_loss: 91.5502\n",
            "Epoch 47/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 97.6232 - val_loss: 90.7299\n",
            "Epoch 48/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 110.3454 - val_loss: 89.9392\n",
            "Epoch 49/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 107.2323 - val_loss: 89.2164\n",
            "Epoch 50/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 97.2939 - val_loss: 88.5547\n",
            "Epoch 51/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 91.7358 - val_loss: 88.0461\n",
            "Epoch 52/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 94.9101 - val_loss: 87.6285\n",
            "Epoch 53/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 102.6039 - val_loss: 87.2425\n",
            "Epoch 54/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 97.3869 - val_loss: 86.8762\n",
            "Epoch 55/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 92.3395 - val_loss: 86.5360\n",
            "Epoch 56/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 80.6493 - val_loss: 86.2108\n",
            "Epoch 57/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 97.2785 - val_loss: 85.9160\n",
            "Epoch 58/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 97.6224 - val_loss: 85.6447\n",
            "Epoch 59/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 85.7370 - val_loss: 85.3881\n",
            "Epoch 60/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 87.5390 - val_loss: 85.1536\n",
            "Epoch 61/1000\n",
            "271/271 [==============================] - 0s 480us/step - loss: 81.2182 - val_loss: 84.9448\n",
            "Epoch 62/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 95.0268 - val_loss: 84.7399\n",
            "Epoch 63/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 84.5867 - val_loss: 84.5400\n",
            "Epoch 64/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 89.0462 - val_loss: 84.3457\n",
            "Epoch 65/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 95.8471 - val_loss: 84.1549\n",
            "Epoch 66/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 92.6015 - val_loss: 83.9687\n",
            "Epoch 67/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 84.2724 - val_loss: 83.7986\n",
            "Epoch 68/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 75.8575 - val_loss: 83.6505\n",
            "Epoch 69/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 76.5646 - val_loss: 83.5146\n",
            "Epoch 70/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 80.3649 - val_loss: 83.3800\n",
            "Epoch 71/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 73.2703 - val_loss: 83.2649\n",
            "Epoch 72/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 77.2324 - val_loss: 83.1497\n",
            "Epoch 73/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 75.8038 - val_loss: 83.0379\n",
            "Epoch 74/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 76.6351 - val_loss: 82.9192\n",
            "Epoch 75/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 80.4656 - val_loss: 82.8081\n",
            "Epoch 76/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 79.4113 - val_loss: 82.6993\n",
            "Epoch 77/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 78.9968 - val_loss: 82.5812\n",
            "Epoch 78/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 75.3946 - val_loss: 82.4665\n",
            "Epoch 79/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 77.8908 - val_loss: 82.3536\n",
            "Epoch 80/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 74.5749 - val_loss: 82.2254\n",
            "Epoch 81/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 72.9589 - val_loss: 82.1037\n",
            "Epoch 82/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 72.2862 - val_loss: 81.9815\n",
            "Epoch 83/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 70.2408 - val_loss: 81.8539\n",
            "Epoch 84/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 70.3603 - val_loss: 81.7107\n",
            "Epoch 85/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 73.2057 - val_loss: 81.5601\n",
            "Epoch 86/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 76.4549 - val_loss: 81.4130\n",
            "Epoch 87/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 67.8572 - val_loss: 81.2693\n",
            "Epoch 88/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 67.3527 - val_loss: 81.1364\n",
            "Epoch 89/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 63.8243 - val_loss: 81.0086\n",
            "Epoch 90/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 69.6841 - val_loss: 80.8824\n",
            "Epoch 91/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 67.7948 - val_loss: 80.7669\n",
            "Epoch 92/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 71.3509 - val_loss: 80.6526\n",
            "Epoch 93/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 71.6459 - val_loss: 80.5384\n",
            "Epoch 94/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 72.0341 - val_loss: 80.4235\n",
            "Epoch 95/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 64.9759 - val_loss: 80.3127\n",
            "Epoch 96/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 68.4560 - val_loss: 80.2133\n",
            "Epoch 97/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 68.8181 - val_loss: 80.1175\n",
            "Epoch 98/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 65.0099 - val_loss: 80.0168\n",
            "Epoch 99/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 64.7502 - val_loss: 79.9206\n",
            "Epoch 100/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 69.5846 - val_loss: 79.8287\n",
            "Epoch 101/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 67.1717 - val_loss: 79.7329\n",
            "Epoch 102/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 68.2393 - val_loss: 79.6426\n",
            "Epoch 103/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 66.6228 - val_loss: 79.5572\n",
            "Epoch 104/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 69.3243 - val_loss: 79.4659\n",
            "Epoch 105/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 67.6068 - val_loss: 79.3794\n",
            "Epoch 106/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 67.4440 - val_loss: 79.3044\n",
            "Epoch 107/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 68.6188 - val_loss: 79.2255\n",
            "Epoch 108/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 69.7008 - val_loss: 79.1465\n",
            "Epoch 109/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 69.0877 - val_loss: 79.0586\n",
            "Epoch 110/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 66.1520 - val_loss: 78.9695\n",
            "Epoch 111/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 66.3575 - val_loss: 78.8807\n",
            "Epoch 112/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 65.2106 - val_loss: 78.7923\n",
            "Epoch 113/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 67.6381 - val_loss: 78.7033\n",
            "Epoch 114/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 70.7293 - val_loss: 78.6065\n",
            "Epoch 115/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 68.3853 - val_loss: 78.5061\n",
            "Epoch 116/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 65.0131 - val_loss: 78.4070\n",
            "Epoch 117/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 67.7482 - val_loss: 78.3133\n",
            "Epoch 118/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 62.5771 - val_loss: 78.2274\n",
            "Epoch 119/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 64.2776 - val_loss: 78.1487\n",
            "Epoch 120/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 64.7572 - val_loss: 78.0831\n",
            "Epoch 121/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 68.0261 - val_loss: 78.0199\n",
            "Epoch 122/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 68.7013 - val_loss: 77.9545\n",
            "Epoch 123/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 61.1113 - val_loss: 77.8888\n",
            "Epoch 124/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 61.6949 - val_loss: 77.8176\n",
            "Epoch 125/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 67.0929 - val_loss: 77.7513\n",
            "Epoch 126/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 68.6094 - val_loss: 77.6897\n",
            "Epoch 127/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 64.7203 - val_loss: 77.6302\n",
            "Epoch 128/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 64.4234 - val_loss: 77.5705\n",
            "Epoch 129/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 63.1916 - val_loss: 77.5068\n",
            "Epoch 130/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 59.7434 - val_loss: 77.4460\n",
            "Epoch 131/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 62.7420 - val_loss: 77.3875\n",
            "Epoch 132/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 62.6484 - val_loss: 77.3077\n",
            "Epoch 133/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 60.8114 - val_loss: 77.2255\n",
            "Epoch 134/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 61.3470 - val_loss: 77.1552\n",
            "Epoch 135/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 58.8630 - val_loss: 77.0575\n",
            "Epoch 136/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 62.8932 - val_loss: 76.9626\n",
            "Epoch 137/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 63.4311 - val_loss: 76.8476\n",
            "Epoch 138/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 62.3290 - val_loss: 76.7460\n",
            "Epoch 139/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 61.0139 - val_loss: 76.6333\n",
            "Epoch 140/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 67.8119 - val_loss: 76.5032\n",
            "Epoch 141/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 65.5424 - val_loss: 76.3673\n",
            "Epoch 142/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 60.8594 - val_loss: 76.2334\n",
            "Epoch 143/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 65.9871 - val_loss: 76.1287\n",
            "Epoch 144/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 66.5770 - val_loss: 76.0189\n",
            "Epoch 145/1000\n",
            "271/271 [==============================] - 0s 479us/step - loss: 62.9383 - val_loss: 75.9164\n",
            "Epoch 146/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 63.6946 - val_loss: 75.8236\n",
            "Epoch 147/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 62.2012 - val_loss: 75.7308\n",
            "Epoch 148/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 60.2628 - val_loss: 75.6293\n",
            "Epoch 149/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 63.9921 - val_loss: 75.5385\n",
            "Epoch 150/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 61.5127 - val_loss: 75.4559\n",
            "Epoch 151/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 62.6432 - val_loss: 75.3855\n",
            "Epoch 152/1000\n",
            "271/271 [==============================] - 0s 506us/step - loss: 62.7397 - val_loss: 75.3178\n",
            "Epoch 153/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 63.1763 - val_loss: 75.2557\n",
            "Epoch 154/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 57.9005 - val_loss: 75.2140\n",
            "Epoch 155/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 57.7220 - val_loss: 75.1729\n",
            "Epoch 156/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 63.0185 - val_loss: 75.1349\n",
            "Epoch 157/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 62.3283 - val_loss: 75.0955\n",
            "Epoch 158/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 62.4311 - val_loss: 75.0505\n",
            "Epoch 159/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 60.6881 - val_loss: 75.0083\n",
            "Epoch 160/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 58.8943 - val_loss: 74.9694\n",
            "Epoch 161/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 59.4037 - val_loss: 74.9291\n",
            "Epoch 162/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 60.0761 - val_loss: 74.8934\n",
            "Epoch 163/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 59.7414 - val_loss: 74.8509\n",
            "Epoch 164/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 62.9898 - val_loss: 74.8001\n",
            "Epoch 165/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 59.8319 - val_loss: 74.7333\n",
            "Epoch 166/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 60.1725 - val_loss: 74.6564\n",
            "Epoch 167/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 58.3691 - val_loss: 74.5856\n",
            "Epoch 168/1000\n",
            "271/271 [==============================] - 0s 506us/step - loss: 58.8838 - val_loss: 74.5049\n",
            "Epoch 169/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 59.1341 - val_loss: 74.4237\n",
            "Epoch 170/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 58.6981 - val_loss: 74.3500\n",
            "Epoch 171/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 62.3501 - val_loss: 74.2667\n",
            "Epoch 172/1000\n",
            "271/271 [==============================] - 0s 509us/step - loss: 56.6548 - val_loss: 74.1858\n",
            "Epoch 173/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 59.6840 - val_loss: 74.1055\n",
            "Epoch 174/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 57.3910 - val_loss: 74.0348\n",
            "Epoch 175/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 57.9019 - val_loss: 73.9642\n",
            "Epoch 176/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 56.4103 - val_loss: 73.8827\n",
            "Epoch 177/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 61.2412 - val_loss: 73.7987\n",
            "Epoch 178/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 58.6310 - val_loss: 73.7220\n",
            "Epoch 179/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 61.2058 - val_loss: 73.6376\n",
            "Epoch 180/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 56.9588 - val_loss: 73.5485\n",
            "Epoch 181/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 56.2078 - val_loss: 73.4573\n",
            "Epoch 182/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 60.1492 - val_loss: 73.3660\n",
            "Epoch 183/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 58.0480 - val_loss: 73.2846\n",
            "Epoch 184/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 62.4979 - val_loss: 73.2576\n",
            "Epoch 185/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 58.7783 - val_loss: 73.2252\n",
            "Epoch 186/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 55.7600 - val_loss: 73.1827\n",
            "Epoch 187/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 54.3393 - val_loss: 73.1456\n",
            "Epoch 188/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 58.2413 - val_loss: 73.0933\n",
            "Epoch 189/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 58.8308 - val_loss: 73.0287\n",
            "Epoch 190/1000\n",
            "271/271 [==============================] - 0s 513us/step - loss: 57.9903 - val_loss: 72.9797\n",
            "Epoch 191/1000\n",
            "271/271 [==============================] - 0s 506us/step - loss: 55.4273 - val_loss: 72.9329\n",
            "Epoch 192/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 58.4818 - val_loss: 72.8895\n",
            "Epoch 193/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 57.5755 - val_loss: 72.8584\n",
            "Epoch 194/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 55.5309 - val_loss: 72.8227\n",
            "Epoch 195/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 56.4654 - val_loss: 72.7854\n",
            "Epoch 196/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 59.0996 - val_loss: 72.7483\n",
            "Epoch 197/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 55.5580 - val_loss: 72.7146\n",
            "Epoch 198/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 54.6216 - val_loss: 72.6839\n",
            "Epoch 199/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 59.0962 - val_loss: 72.6566\n",
            "Epoch 200/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 58.8612 - val_loss: 72.6441\n",
            "Epoch 201/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 57.5644 - val_loss: 72.6166\n",
            "Epoch 202/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 56.7143 - val_loss: 72.5961\n",
            "Epoch 203/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 56.4861 - val_loss: 72.5862\n",
            "Epoch 204/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 59.0313 - val_loss: 72.5792\n",
            "Epoch 205/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 58.6583 - val_loss: 72.5708\n",
            "Epoch 206/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 56.2065 - val_loss: 72.5717\n",
            "Epoch 207/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 55.7030 - val_loss: 72.5662\n",
            "Epoch 208/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 59.9836 - val_loss: 72.5591\n",
            "Epoch 209/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 56.6641 - val_loss: 72.5448\n",
            "Epoch 210/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 57.2857 - val_loss: 72.5193\n",
            "Epoch 211/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 59.6531 - val_loss: 72.4985\n",
            "Epoch 212/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 54.3358 - val_loss: 72.4905\n",
            "Epoch 213/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 58.3553 - val_loss: 72.4881\n",
            "Epoch 214/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 58.2001 - val_loss: 72.4822\n",
            "Epoch 215/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 52.1580 - val_loss: 72.4785\n",
            "Epoch 216/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 57.1442 - val_loss: 72.4818\n",
            "Epoch 217/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 57.0453 - val_loss: 72.4860\n",
            "Epoch 218/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 52.9143 - val_loss: 72.5006\n",
            "Epoch 219/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 56.6364 - val_loss: 72.5162\n",
            "Epoch 220/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 53.3502 - val_loss: 72.5267\n",
            "Epoch 221/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 56.1726 - val_loss: 72.5320\n",
            "Epoch 222/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 54.2771 - val_loss: 72.5345\n",
            "Epoch 223/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 56.6348 - val_loss: 72.5370\n",
            "Epoch 224/1000\n",
            "271/271 [==============================] - 0s 508us/step - loss: 52.8353 - val_loss: 72.5353\n",
            "Epoch 225/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 53.7737 - val_loss: 72.5274\n",
            "Epoch 226/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 56.1784 - val_loss: 72.5250\n",
            "Epoch 227/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 53.1612 - val_loss: 72.5037\n",
            "Epoch 228/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 54.7885 - val_loss: 72.4742\n",
            "Epoch 229/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 52.3369 - val_loss: 72.4415\n",
            "Epoch 230/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 56.7848 - val_loss: 72.4173\n",
            "Epoch 231/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 55.4500 - val_loss: 72.3983\n",
            "Epoch 232/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 54.9459 - val_loss: 72.3799\n",
            "Epoch 233/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 51.2244 - val_loss: 72.3620\n",
            "Epoch 234/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 54.7278 - val_loss: 72.3517\n",
            "Epoch 235/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 50.6970 - val_loss: 72.3469\n",
            "Epoch 236/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 55.8078 - val_loss: 72.3380\n",
            "Epoch 237/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 55.1944 - val_loss: 72.3346\n",
            "Epoch 238/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 54.4756 - val_loss: 72.3248\n",
            "Epoch 239/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 52.3834 - val_loss: 72.3161\n",
            "Epoch 240/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 52.7509 - val_loss: 72.3030\n",
            "Epoch 241/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 52.4938 - val_loss: 72.2952\n",
            "Epoch 242/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 51.3822 - val_loss: 72.2857\n",
            "Epoch 243/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 52.3745 - val_loss: 72.2803\n",
            "Epoch 244/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 51.6583 - val_loss: 72.2723\n",
            "Epoch 245/1000\n",
            "271/271 [==============================] - 0s 480us/step - loss: 53.5714 - val_loss: 72.2529\n",
            "Epoch 246/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 51.8413 - val_loss: 72.2341\n",
            "Epoch 247/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 51.1102 - val_loss: 72.2167\n",
            "Epoch 248/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 51.3522 - val_loss: 72.2039\n",
            "Epoch 249/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 55.7069 - val_loss: 72.1940\n",
            "Epoch 250/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 51.1981 - val_loss: 72.1950\n",
            "Epoch 251/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 52.2380 - val_loss: 72.1987\n",
            "Epoch 252/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 51.9321 - val_loss: 72.2036\n",
            "Epoch 253/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 50.3036 - val_loss: 72.2118\n",
            "Epoch 254/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 51.0161 - val_loss: 72.2264\n",
            "Epoch 255/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 51.8135 - val_loss: 72.2318\n",
            "Epoch 256/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 50.5040 - val_loss: 72.2237\n",
            "Epoch 257/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 50.8608 - val_loss: 72.2033\n",
            "Epoch 258/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 51.1194 - val_loss: 72.1744\n",
            "Epoch 259/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 50.7687 - val_loss: 72.1531\n",
            "Epoch 260/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 50.5739 - val_loss: 72.1348\n",
            "Epoch 261/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 52.1447 - val_loss: 72.1404\n",
            "Epoch 262/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 51.0066 - val_loss: 72.1424\n",
            "Epoch 263/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 52.8638 - val_loss: 72.1415\n",
            "Epoch 264/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 48.2216 - val_loss: 72.1425\n",
            "Epoch 265/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 49.7538 - val_loss: 72.1142\n",
            "Epoch 266/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 49.8589 - val_loss: 72.0822\n",
            "Epoch 267/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 51.3745 - val_loss: 72.0394\n",
            "Epoch 268/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 49.3496 - val_loss: 71.9842\n",
            "Epoch 269/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 49.9590 - val_loss: 71.9159\n",
            "Epoch 270/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 51.0543 - val_loss: 71.8817\n",
            "Epoch 271/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 49.5410 - val_loss: 71.8590\n",
            "Epoch 272/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 48.3016 - val_loss: 71.8372\n",
            "Epoch 273/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 51.6404 - val_loss: 71.7964\n",
            "Epoch 274/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 50.7794 - val_loss: 71.7674\n",
            "Epoch 275/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 49.0292 - val_loss: 71.7189\n",
            "Epoch 276/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 48.7112 - val_loss: 71.6971\n",
            "Epoch 277/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 51.3621 - val_loss: 71.6953\n",
            "Epoch 278/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 52.2949 - val_loss: 71.6936\n",
            "Epoch 279/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 49.3869 - val_loss: 71.6984\n",
            "Epoch 280/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 47.2212 - val_loss: 71.6987\n",
            "Epoch 281/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 49.1126 - val_loss: 71.6894\n",
            "Epoch 282/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 49.8243 - val_loss: 71.6403\n",
            "Epoch 283/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 46.7768 - val_loss: 71.6063\n",
            "Epoch 284/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 47.2271 - val_loss: 71.5807\n",
            "Epoch 285/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 46.8896 - val_loss: 71.5756\n",
            "Epoch 286/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 48.2455 - val_loss: 71.5676\n",
            "Epoch 287/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 51.0348 - val_loss: 71.5494\n",
            "Epoch 288/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 46.8979 - val_loss: 71.5427\n",
            "Epoch 289/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 46.8236 - val_loss: 71.5406\n",
            "Epoch 290/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 48.2592 - val_loss: 71.5357\n",
            "Epoch 291/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 46.4989 - val_loss: 71.5380\n",
            "Epoch 292/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 46.7715 - val_loss: 71.5376\n",
            "Epoch 293/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 44.8426 - val_loss: 71.5233\n",
            "Epoch 294/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 47.3226 - val_loss: 71.4972\n",
            "Epoch 295/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 48.8765 - val_loss: 71.4605\n",
            "Epoch 296/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 46.2688 - val_loss: 71.4084\n",
            "Epoch 297/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 49.0924 - val_loss: 71.4074\n",
            "Epoch 298/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 47.1360 - val_loss: 71.4354\n",
            "Epoch 299/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 46.7750 - val_loss: 71.4431\n",
            "Epoch 300/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 46.3521 - val_loss: 71.4543\n",
            "Epoch 301/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 45.3268 - val_loss: 71.4890\n",
            "Epoch 302/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 45.3976 - val_loss: 71.5314\n",
            "Epoch 303/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 45.8923 - val_loss: 71.5732\n",
            "Epoch 304/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 44.4263 - val_loss: 71.6450\n",
            "Epoch 305/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 46.6105 - val_loss: 71.7138\n",
            "Epoch 306/1000\n",
            "271/271 [==============================] - 0s 519us/step - loss: 43.8742 - val_loss: 71.7648\n",
            "Epoch 307/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 43.9759 - val_loss: 71.8046\n",
            "Epoch 308/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 46.1846 - val_loss: 71.8435\n",
            "Epoch 309/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 45.6243 - val_loss: 71.8858\n",
            "Epoch 310/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 45.6731 - val_loss: 71.8810\n",
            "Epoch 311/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 44.7117 - val_loss: 71.8950\n",
            "Epoch 312/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 42.7874 - val_loss: 71.9464\n",
            "Epoch 313/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 43.2318 - val_loss: 71.9923\n",
            "Epoch 314/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 45.5192 - val_loss: 72.1874\n",
            "Epoch 315/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 43.9078 - val_loss: 72.2152\n",
            "Epoch 316/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 43.0522 - val_loss: 72.2489\n",
            "Epoch 317/1000\n",
            "271/271 [==============================] - 0s 481us/step - loss: 44.3209 - val_loss: 72.3006\n",
            "Epoch 318/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 43.4795 - val_loss: 72.3656\n",
            "Epoch 319/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 45.2731 - val_loss: 72.4232\n",
            "Epoch 320/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 46.0169 - val_loss: 72.4823\n",
            "Epoch 321/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 45.3729 - val_loss: 72.5328\n",
            "Epoch 322/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 42.9860 - val_loss: 72.5744\n",
            "Epoch 323/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 43.3581 - val_loss: 72.5771\n",
            "Epoch 324/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 44.1948 - val_loss: 72.5465\n",
            "Epoch 325/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 43.8325 - val_loss: 72.5064\n",
            "Epoch 326/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 43.9264 - val_loss: 72.4568\n",
            "Epoch 327/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 43.4631 - val_loss: 72.4043\n",
            "Epoch 328/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 43.2685 - val_loss: 72.3749\n",
            "Epoch 329/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 42.9824 - val_loss: 72.3534\n",
            "Epoch 330/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 43.2545 - val_loss: 72.3587\n",
            "Epoch 331/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 44.7217 - val_loss: 72.3152\n",
            "Epoch 332/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 44.0982 - val_loss: 72.0757\n",
            "Epoch 333/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 42.4667 - val_loss: 72.1694\n",
            "Epoch 334/1000\n",
            "271/271 [==============================] - 0s 510us/step - loss: 42.9754 - val_loss: 72.2700\n",
            "Epoch 335/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 43.5839 - val_loss: 72.2574\n",
            "Epoch 336/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 43.0655 - val_loss: 72.2751\n",
            "Epoch 337/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 43.8020 - val_loss: 72.2807\n",
            "Epoch 338/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 41.5272 - val_loss: 72.2791\n",
            "Epoch 339/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 41.9027 - val_loss: 72.2635\n",
            "Epoch 340/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 41.9303 - val_loss: 72.2367\n",
            "Epoch 341/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 40.5608 - val_loss: 72.2165\n",
            "Epoch 342/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 41.3942 - val_loss: 72.1960\n",
            "Epoch 343/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 40.5528 - val_loss: 72.1451\n",
            "Epoch 344/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 39.6328 - val_loss: 72.0958\n",
            "Epoch 345/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 42.0375 - val_loss: 72.0675\n",
            "Epoch 346/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 42.4809 - val_loss: 72.0489\n",
            "Epoch 347/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 41.3224 - val_loss: 72.0434\n",
            "Epoch 348/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 41.0434 - val_loss: 72.0560\n",
            "Epoch 349/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 42.3196 - val_loss: 72.1975\n",
            "Epoch 350/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 39.8474 - val_loss: 72.4583\n",
            "Epoch 351/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 39.1295 - val_loss: 72.4326\n",
            "Epoch 352/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 41.0077 - val_loss: 72.3879\n",
            "Epoch 353/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 40.7557 - val_loss: 72.3300\n",
            "Epoch 354/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 41.4675 - val_loss: 72.2791\n",
            "Epoch 355/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 44.0364 - val_loss: 72.2272\n",
            "Epoch 356/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 39.5896 - val_loss: 72.1754\n",
            "Epoch 357/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 40.4126 - val_loss: 71.9524\n",
            "Epoch 358/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 41.5214 - val_loss: 71.2445\n",
            "Epoch 359/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 40.8230 - val_loss: 71.3488\n",
            "Epoch 360/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 41.6539 - val_loss: 71.7080\n",
            "Epoch 361/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 41.4694 - val_loss: 71.4398\n",
            "Epoch 362/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 40.8206 - val_loss: 71.1299\n",
            "Epoch 363/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 40.7967 - val_loss: 70.7796\n",
            "Epoch 364/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 39.7770 - val_loss: 70.9871\n",
            "Epoch 365/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 43.2426 - val_loss: 71.2456\n",
            "Epoch 366/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 43.1136 - val_loss: 71.4030\n",
            "Epoch 367/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 41.0122 - val_loss: 71.3557\n",
            "Epoch 368/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 40.8459 - val_loss: 71.3162\n",
            "Epoch 369/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 38.5810 - val_loss: 71.2829\n",
            "Epoch 370/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 41.2373 - val_loss: 71.2470\n",
            "Epoch 371/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 39.6425 - val_loss: 71.2292\n",
            "Epoch 372/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 39.1525 - val_loss: 71.1837\n",
            "Epoch 373/1000\n",
            "271/271 [==============================] - 0s 479us/step - loss: 38.8920 - val_loss: 71.1055\n",
            "Epoch 374/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 40.3620 - val_loss: 71.0226\n",
            "Epoch 375/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 39.5428 - val_loss: 70.9795\n",
            "Epoch 376/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 41.8265 - val_loss: 70.9559\n",
            "Epoch 377/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 38.1446 - val_loss: 70.9347\n",
            "Epoch 378/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 38.4228 - val_loss: 70.8884\n",
            "Epoch 379/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 41.1748 - val_loss: 70.8401\n",
            "Epoch 380/1000\n",
            "271/271 [==============================] - 0s 519us/step - loss: 38.7786 - val_loss: 70.8729\n",
            "Epoch 381/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 40.5299 - val_loss: 70.8412\n",
            "Epoch 382/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 38.1426 - val_loss: 70.8420\n",
            "Epoch 383/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 36.9248 - val_loss: 70.8379\n",
            "Epoch 384/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 39.4104 - val_loss: 70.8002\n",
            "Epoch 385/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 35.3388 - val_loss: 70.7617\n",
            "Epoch 386/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 38.7340 - val_loss: 70.7545\n",
            "Epoch 387/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 37.4134 - val_loss: 70.7387\n",
            "Epoch 388/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 38.5208 - val_loss: 70.6705\n",
            "Epoch 389/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 35.6635 - val_loss: 70.5750\n",
            "Epoch 390/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 37.8445 - val_loss: 70.4703\n",
            "Epoch 391/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 37.8365 - val_loss: 70.4616\n",
            "Epoch 392/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 37.1039 - val_loss: 70.5443\n",
            "Epoch 393/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 38.6281 - val_loss: 70.5456\n",
            "Epoch 394/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 37.2841 - val_loss: 70.2814\n",
            "Epoch 395/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 39.0106 - val_loss: 70.5115\n",
            "Epoch 396/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 38.1699 - val_loss: 70.5001\n",
            "Epoch 397/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 38.9225 - val_loss: 70.4958\n",
            "Epoch 398/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 36.7313 - val_loss: 70.5351\n",
            "Epoch 399/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 38.9856 - val_loss: 70.5547\n",
            "Epoch 400/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 36.7624 - val_loss: 70.5614\n",
            "Epoch 401/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 37.5236 - val_loss: 70.5519\n",
            "Epoch 402/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 37.2427 - val_loss: 70.5335\n",
            "Epoch 403/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 32.4627 - val_loss: 70.5097\n",
            "Epoch 404/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 36.1814 - val_loss: 70.5283\n",
            "Epoch 405/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 35.4726 - val_loss: 70.5828\n",
            "Epoch 406/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 34.5280 - val_loss: 70.5429\n",
            "Epoch 407/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 36.9293 - val_loss: 70.4672\n",
            "Epoch 408/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 35.8031 - val_loss: 70.3584\n",
            "Epoch 409/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 38.8248 - val_loss: 70.2956\n",
            "Epoch 410/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 36.7052 - val_loss: 70.2437\n",
            "Epoch 411/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 37.9960 - val_loss: 70.2217\n",
            "Epoch 412/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 37.4412 - val_loss: 70.2167\n",
            "Epoch 413/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 35.0583 - val_loss: 70.2041\n",
            "Epoch 414/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 37.9509 - val_loss: 70.2215\n",
            "Epoch 415/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 37.1013 - val_loss: 70.2325\n",
            "Epoch 416/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 35.3922 - val_loss: 70.1943\n",
            "Epoch 417/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 33.9605 - val_loss: 70.1436\n",
            "Epoch 418/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 38.8629 - val_loss: 70.0996\n",
            "Epoch 419/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 36.0194 - val_loss: 70.0618\n",
            "Epoch 420/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 34.3645 - val_loss: 70.0736\n",
            "Epoch 421/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 35.7136 - val_loss: 70.0853\n",
            "Epoch 422/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 35.1024 - val_loss: 70.0805\n",
            "Epoch 423/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 34.9568 - val_loss: 70.0529\n",
            "Epoch 424/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 34.1163 - val_loss: 70.0382\n",
            "Epoch 425/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 37.6780 - val_loss: 70.0367\n",
            "Epoch 426/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 34.3072 - val_loss: 70.0126\n",
            "Epoch 427/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 36.1386 - val_loss: 69.9803\n",
            "Epoch 428/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 37.0407 - val_loss: 69.9550\n",
            "Epoch 429/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 37.0000 - val_loss: 69.9314\n",
            "Epoch 430/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 35.4834 - val_loss: 69.9240\n",
            "Epoch 431/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 36.3094 - val_loss: 69.8979\n",
            "Epoch 432/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 36.3501 - val_loss: 69.8754\n",
            "Epoch 433/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 33.1072 - val_loss: 69.7971\n",
            "Epoch 434/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 34.4902 - val_loss: 69.6970\n",
            "Epoch 435/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.5034 - val_loss: 69.5862\n",
            "Epoch 436/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 33.1408 - val_loss: 69.4749\n",
            "Epoch 437/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 34.6805 - val_loss: 69.4541\n",
            "Epoch 438/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 35.0529 - val_loss: 69.3957\n",
            "Epoch 439/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 33.5767 - val_loss: 69.3218\n",
            "Epoch 440/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 33.4713 - val_loss: 69.2252\n",
            "Epoch 441/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 34.7152 - val_loss: 69.1643\n",
            "Epoch 442/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 35.9025 - val_loss: 69.1049\n",
            "Epoch 443/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 36.8764 - val_loss: 69.0613\n",
            "Epoch 444/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 37.8515 - val_loss: 69.0312\n",
            "Epoch 445/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.2268 - val_loss: 69.0063\n",
            "Epoch 446/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 35.9670 - val_loss: 68.9730\n",
            "Epoch 447/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 34.2301 - val_loss: 69.0212\n",
            "Epoch 448/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 36.2515 - val_loss: 69.0591\n",
            "Epoch 449/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 34.3121 - val_loss: 69.0209\n",
            "Epoch 450/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 32.4509 - val_loss: 68.9025\n",
            "Epoch 451/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 36.2238 - val_loss: 68.7529\n",
            "Epoch 452/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 35.6762 - val_loss: 68.6101\n",
            "Epoch 453/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 33.5924 - val_loss: 68.4626\n",
            "Epoch 454/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 34.6543 - val_loss: 68.2809\n",
            "Epoch 455/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 33.4609 - val_loss: 68.1351\n",
            "Epoch 456/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 34.0808 - val_loss: 68.0598\n",
            "Epoch 457/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 34.7277 - val_loss: 68.0272\n",
            "Epoch 458/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 36.7345 - val_loss: 68.0181\n",
            "Epoch 459/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 34.9308 - val_loss: 68.0265\n",
            "Epoch 460/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 35.0694 - val_loss: 68.0709\n",
            "Epoch 461/1000\n",
            "271/271 [==============================] - 0s 509us/step - loss: 32.8563 - val_loss: 68.0491\n",
            "Epoch 462/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 31.6458 - val_loss: 68.0071\n",
            "Epoch 463/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 34.2626 - val_loss: 67.9188\n",
            "Epoch 464/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 31.4626 - val_loss: 67.8217\n",
            "Epoch 465/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 32.2373 - val_loss: 67.7164\n",
            "Epoch 466/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.2234 - val_loss: 67.6192\n",
            "Epoch 467/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 32.3552 - val_loss: 67.5824\n",
            "Epoch 468/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.4567 - val_loss: 67.5623\n",
            "Epoch 469/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 33.7964 - val_loss: 67.5728\n",
            "Epoch 470/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 34.4016 - val_loss: 67.6348\n",
            "Epoch 471/1000\n",
            "271/271 [==============================] - 0s 513us/step - loss: 34.9809 - val_loss: 67.6597\n",
            "Epoch 472/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 31.6921 - val_loss: 67.7087\n",
            "Epoch 473/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 31.9005 - val_loss: 67.6841\n",
            "Epoch 474/1000\n",
            "271/271 [==============================] - 0s 519us/step - loss: 32.0206 - val_loss: 67.5701\n",
            "Epoch 475/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 32.6398 - val_loss: 67.4856\n",
            "Epoch 476/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.8533 - val_loss: 67.4158\n",
            "Epoch 477/1000\n",
            "271/271 [==============================] - 0s 514us/step - loss: 31.7206 - val_loss: 67.3679\n",
            "Epoch 478/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.2199 - val_loss: 67.3469\n",
            "Epoch 479/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 32.9091 - val_loss: 67.3730\n",
            "Epoch 480/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 32.9045 - val_loss: 67.3426\n",
            "Epoch 481/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 32.5915 - val_loss: 67.3624\n",
            "Epoch 482/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 32.6116 - val_loss: 67.3379\n",
            "Epoch 483/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 30.8490 - val_loss: 67.3224\n",
            "Epoch 484/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 31.6914 - val_loss: 67.2639\n",
            "Epoch 485/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 32.0555 - val_loss: 67.1678\n",
            "Epoch 486/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 30.0634 - val_loss: 67.0485\n",
            "Epoch 487/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 31.9632 - val_loss: 66.9477\n",
            "Epoch 488/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 32.9742 - val_loss: 66.8819\n",
            "Epoch 489/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 32.8018 - val_loss: 66.8567\n",
            "Epoch 490/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 32.6161 - val_loss: 66.8258\n",
            "Epoch 491/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.8867 - val_loss: 66.7533\n",
            "Epoch 492/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 32.7625 - val_loss: 66.6979\n",
            "Epoch 493/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 32.3883 - val_loss: 66.6527\n",
            "Epoch 494/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 31.5895 - val_loss: 66.5045\n",
            "Epoch 495/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 32.2405 - val_loss: 66.3072\n",
            "Epoch 496/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 31.8755 - val_loss: 66.1399\n",
            "Epoch 497/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 31.7643 - val_loss: 65.9949\n",
            "Epoch 498/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 32.6702 - val_loss: 65.7657\n",
            "Epoch 499/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.9824 - val_loss: 65.5742\n",
            "Epoch 500/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 30.4291 - val_loss: 65.4423\n",
            "Epoch 501/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 32.9253 - val_loss: 65.2910\n",
            "Epoch 502/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 30.4339 - val_loss: 65.2370\n",
            "Epoch 503/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 29.9376 - val_loss: 65.2004\n",
            "Epoch 504/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 33.1056 - val_loss: 65.1453\n",
            "Epoch 505/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 35.2622 - val_loss: 65.1432\n",
            "Epoch 506/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 32.4939 - val_loss: 65.1732\n",
            "Epoch 507/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 34.2268 - val_loss: 65.2211\n",
            "Epoch 508/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 33.2155 - val_loss: 65.1970\n",
            "Epoch 509/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 31.9763 - val_loss: 65.1465\n",
            "Epoch 510/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 32.1479 - val_loss: 65.0927\n",
            "Epoch 511/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 31.7106 - val_loss: 64.9823\n",
            "Epoch 512/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 29.7204 - val_loss: 64.8562\n",
            "Epoch 513/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 31.9281 - val_loss: 64.6893\n",
            "Epoch 514/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.9731 - val_loss: 64.5203\n",
            "Epoch 515/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 33.0554 - val_loss: 64.3806\n",
            "Epoch 516/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 31.6358 - val_loss: 64.3402\n",
            "Epoch 517/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 32.8628 - val_loss: 64.2898\n",
            "Epoch 518/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 31.5861 - val_loss: 64.2819\n",
            "Epoch 519/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.2448 - val_loss: 64.3182\n",
            "Epoch 520/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 30.8729 - val_loss: 64.3480\n",
            "Epoch 521/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 29.1687 - val_loss: 64.3808\n",
            "Epoch 522/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 30.8088 - val_loss: 64.4021\n",
            "Epoch 523/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 33.0553 - val_loss: 64.4309\n",
            "Epoch 524/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 32.5474 - val_loss: 64.4584\n",
            "Epoch 525/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 31.6230 - val_loss: 64.4225\n",
            "Epoch 526/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 33.3101 - val_loss: 64.3394\n",
            "Epoch 527/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 31.8141 - val_loss: 64.2275\n",
            "Epoch 528/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 29.4466 - val_loss: 64.1274\n",
            "Epoch 529/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 31.1347 - val_loss: 64.0122\n",
            "Epoch 530/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 29.5085 - val_loss: 63.9100\n",
            "Epoch 531/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 31.4655 - val_loss: 63.7877\n",
            "Epoch 532/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 29.5276 - val_loss: 63.6116\n",
            "Epoch 533/1000\n",
            "271/271 [==============================] - 0s 482us/step - loss: 32.5560 - val_loss: 63.4255\n",
            "Epoch 534/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 30.8608 - val_loss: 63.2966\n",
            "Epoch 535/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 31.8834 - val_loss: 63.1861\n",
            "Epoch 536/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.2316 - val_loss: 63.0488\n",
            "Epoch 537/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 29.6542 - val_loss: 62.8728\n",
            "Epoch 538/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 30.0768 - val_loss: 62.6945\n",
            "Epoch 539/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 28.0046 - val_loss: 62.5152\n",
            "Epoch 540/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.4606 - val_loss: 62.4388\n",
            "Epoch 541/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 29.4456 - val_loss: 62.3874\n",
            "Epoch 542/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 28.7036 - val_loss: 62.2565\n",
            "Epoch 543/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 30.7384 - val_loss: 62.0631\n",
            "Epoch 544/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 29.3601 - val_loss: 61.9535\n",
            "Epoch 545/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 30.9120 - val_loss: 61.8115\n",
            "Epoch 546/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.8125 - val_loss: 61.6378\n",
            "Epoch 547/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 30.1947 - val_loss: 61.4439\n",
            "Epoch 548/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 29.8551 - val_loss: 61.2033\n",
            "Epoch 549/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 31.6334 - val_loss: 60.8681\n",
            "Epoch 550/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 30.9004 - val_loss: 60.4709\n",
            "Epoch 551/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 33.0197 - val_loss: 60.1205\n",
            "Epoch 552/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 30.0660 - val_loss: 59.8887\n",
            "Epoch 553/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 33.8713 - val_loss: 59.8277\n",
            "Epoch 554/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.5642 - val_loss: 59.7868\n",
            "Epoch 555/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 30.4720 - val_loss: 59.8291\n",
            "Epoch 556/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 31.3487 - val_loss: 59.8508\n",
            "Epoch 557/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 29.4198 - val_loss: 60.0456\n",
            "Epoch 558/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 31.0426 - val_loss: 60.2779\n",
            "Epoch 559/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.9455 - val_loss: 60.5024\n",
            "Epoch 560/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 30.3383 - val_loss: 60.6796\n",
            "Epoch 561/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 31.5403 - val_loss: 60.5100\n",
            "Epoch 562/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 33.4910 - val_loss: 60.3411\n",
            "Epoch 563/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 33.1401 - val_loss: 60.3203\n",
            "Epoch 564/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 30.8914 - val_loss: 60.3456\n",
            "Epoch 565/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 28.3270 - val_loss: 60.3999\n",
            "Epoch 566/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 31.2957 - val_loss: 60.4375\n",
            "Epoch 567/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 30.0058 - val_loss: 60.4763\n",
            "Epoch 568/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 31.3339 - val_loss: 60.4036\n",
            "Epoch 569/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 30.7480 - val_loss: 60.4811\n",
            "Epoch 570/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 28.8015 - val_loss: 60.4811\n",
            "Epoch 571/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 28.9874 - val_loss: 60.6014\n",
            "Epoch 572/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 30.4541 - val_loss: 60.6718\n",
            "Epoch 573/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 30.3917 - val_loss: 60.5483\n",
            "Epoch 574/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 29.4839 - val_loss: 60.3508\n",
            "Epoch 575/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 29.2643 - val_loss: 60.1269\n",
            "Epoch 576/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 31.7453 - val_loss: 59.7520\n",
            "Epoch 577/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 30.8759 - val_loss: 59.5671\n",
            "Epoch 578/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 31.9348 - val_loss: 59.4262\n",
            "Epoch 579/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 29.5375 - val_loss: 59.3539\n",
            "Epoch 580/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 30.0841 - val_loss: 59.3539\n",
            "Epoch 581/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 30.3600 - val_loss: 59.3944\n",
            "Epoch 582/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 30.2563 - val_loss: 59.4453\n",
            "Epoch 583/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 29.3240 - val_loss: 59.7041\n",
            "Epoch 584/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 28.3310 - val_loss: 60.0505\n",
            "Epoch 585/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 29.4678 - val_loss: 60.4874\n",
            "Epoch 586/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 27.9248 - val_loss: 60.7707\n",
            "Epoch 587/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 30.3500 - val_loss: 61.0233\n",
            "Epoch 588/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 27.8549 - val_loss: 61.1093\n",
            "Epoch 589/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 29.5494 - val_loss: 61.0621\n",
            "Epoch 590/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 28.7789 - val_loss: 60.8527\n",
            "Epoch 591/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 29.7993 - val_loss: 60.3918\n",
            "Epoch 592/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 29.6747 - val_loss: 59.8148\n",
            "Epoch 593/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 29.3267 - val_loss: 59.2399\n",
            "Epoch 594/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 28.4978 - val_loss: 58.8870\n",
            "Epoch 595/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 30.4542 - val_loss: 58.5706\n",
            "Epoch 596/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 29.3581 - val_loss: 58.1999\n",
            "Epoch 597/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 28.5822 - val_loss: 57.9009\n",
            "Epoch 598/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 27.6496 - val_loss: 57.6166\n",
            "Epoch 599/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 30.1938 - val_loss: 57.0307\n",
            "Epoch 600/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 28.5030 - val_loss: 56.4191\n",
            "Epoch 601/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 30.5464 - val_loss: 55.6822\n",
            "Epoch 602/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 30.9729 - val_loss: 54.9786\n",
            "Epoch 603/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 28.4034 - val_loss: 54.6962\n",
            "Epoch 604/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 27.8571 - val_loss: 54.3765\n",
            "Epoch 605/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 27.8434 - val_loss: 54.0770\n",
            "Epoch 606/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 30.9051 - val_loss: 53.9251\n",
            "Epoch 607/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 31.4589 - val_loss: 53.8506\n",
            "Epoch 608/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 29.3157 - val_loss: 53.7258\n",
            "Epoch 609/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 29.4849 - val_loss: 53.6405\n",
            "Epoch 610/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 29.5456 - val_loss: 53.5968\n",
            "Epoch 611/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 29.2765 - val_loss: 53.5425\n",
            "Epoch 612/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 27.6901 - val_loss: 53.4940\n",
            "Epoch 613/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 28.6700 - val_loss: 53.5147\n",
            "Epoch 614/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 27.6110 - val_loss: 53.4333\n",
            "Epoch 615/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 27.9175 - val_loss: 53.3870\n",
            "Epoch 616/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 31.8750 - val_loss: 53.3731\n",
            "Epoch 617/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 29.2206 - val_loss: 53.4038\n",
            "Epoch 618/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 29.3671 - val_loss: 53.3564\n",
            "Epoch 619/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 28.5618 - val_loss: 53.2058\n",
            "Epoch 620/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 27.8134 - val_loss: 53.0276\n",
            "Epoch 621/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 30.0855 - val_loss: 52.8542\n",
            "Epoch 622/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 30.2734 - val_loss: 52.8236\n",
            "Epoch 623/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 27.7905 - val_loss: 52.9425\n",
            "Epoch 624/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 26.4556 - val_loss: 53.0378\n",
            "Epoch 625/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 30.5112 - val_loss: 53.2163\n",
            "Epoch 626/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 28.7365 - val_loss: 53.3610\n",
            "Epoch 627/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 27.4063 - val_loss: 53.5221\n",
            "Epoch 628/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 28.2680 - val_loss: 53.6196\n",
            "Epoch 629/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 28.6587 - val_loss: 53.6802\n",
            "Epoch 630/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 29.6490 - val_loss: 53.7091\n",
            "Epoch 631/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 27.2558 - val_loss: 53.7016\n",
            "Epoch 632/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 26.9599 - val_loss: 53.7418\n",
            "Epoch 633/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 26.7362 - val_loss: 53.7103\n",
            "Epoch 634/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 28.1917 - val_loss: 53.7597\n",
            "Epoch 635/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 27.9928 - val_loss: 53.8512\n",
            "Epoch 636/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 26.5143 - val_loss: 53.9886\n",
            "Epoch 637/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 28.5793 - val_loss: 54.0809\n",
            "Epoch 638/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 27.5701 - val_loss: 54.1203\n",
            "Epoch 639/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 26.5405 - val_loss: 54.0785\n",
            "Epoch 640/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 27.7642 - val_loss: 54.1958\n",
            "Epoch 641/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 26.5757 - val_loss: 54.3795\n",
            "Epoch 642/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 28.5974 - val_loss: 54.6229\n",
            "Epoch 643/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 28.3976 - val_loss: 54.8522\n",
            "Epoch 644/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 29.1594 - val_loss: 54.9173\n",
            "Epoch 645/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 25.5257 - val_loss: 54.9041\n",
            "Epoch 646/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 28.4041 - val_loss: 54.8985\n",
            "Epoch 647/1000\n",
            "271/271 [==============================] - 0s 508us/step - loss: 26.4940 - val_loss: 55.0244\n",
            "Epoch 648/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 26.4209 - val_loss: 55.0421\n",
            "Epoch 649/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 29.4869 - val_loss: 55.0484\n",
            "Epoch 650/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 26.9450 - val_loss: 54.9249\n",
            "Epoch 651/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 27.4340 - val_loss: 54.6778\n",
            "Epoch 652/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 28.8356 - val_loss: 54.4297\n",
            "Epoch 653/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 26.5929 - val_loss: 54.1210\n",
            "Epoch 654/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 25.9293 - val_loss: 53.9834\n",
            "Epoch 655/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 27.3388 - val_loss: 53.8099\n",
            "Epoch 656/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 25.8415 - val_loss: 53.5611\n",
            "Epoch 657/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 27.1580 - val_loss: 53.0734\n",
            "Epoch 658/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 26.1391 - val_loss: 52.8540\n",
            "Epoch 659/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 26.1173 - val_loss: 52.8278\n",
            "Epoch 660/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 26.9371 - val_loss: 52.8872\n",
            "Epoch 661/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 26.7098 - val_loss: 52.9666\n",
            "Epoch 662/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 26.9237 - val_loss: 53.0734\n",
            "Epoch 663/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 24.3209 - val_loss: 53.3755\n",
            "Epoch 664/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 26.8718 - val_loss: 53.7871\n",
            "Epoch 665/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 28.2749 - val_loss: 54.2046\n",
            "Epoch 666/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 25.5825 - val_loss: 54.4000\n",
            "Epoch 667/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 26.6496 - val_loss: 54.5550\n",
            "Epoch 668/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 26.8778 - val_loss: 54.7827\n",
            "Epoch 669/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 25.8202 - val_loss: 54.9198\n",
            "Epoch 670/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 25.6884 - val_loss: 55.3491\n",
            "Epoch 671/1000\n",
            "271/271 [==============================] - 0s 509us/step - loss: 25.9243 - val_loss: 55.8075\n",
            "Epoch 672/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 27.9581 - val_loss: 56.1364\n",
            "Epoch 673/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 26.7537 - val_loss: 56.3687\n",
            "Epoch 674/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 24.7460 - val_loss: 56.6554\n",
            "Epoch 675/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 25.7764 - val_loss: 56.8314\n",
            "Epoch 676/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 25.9132 - val_loss: 56.9843\n",
            "Epoch 677/1000\n",
            "271/271 [==============================] - 0s 507us/step - loss: 26.7040 - val_loss: 56.9122\n",
            "Epoch 678/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 25.2226 - val_loss: 56.7066\n",
            "Epoch 679/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 26.1588 - val_loss: 56.3749\n",
            "Epoch 680/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 27.3301 - val_loss: 56.0489\n",
            "Epoch 681/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 27.4697 - val_loss: 55.9557\n",
            "Epoch 682/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 28.0391 - val_loss: 55.9649\n",
            "Epoch 683/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 23.8498 - val_loss: 56.1881\n",
            "Epoch 684/1000\n",
            "271/271 [==============================] - 0s 508us/step - loss: 26.1744 - val_loss: 56.5684\n",
            "Epoch 685/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 26.5334 - val_loss: 57.1319\n",
            "Epoch 686/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 26.6380 - val_loss: 57.6602\n",
            "Epoch 687/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 26.2136 - val_loss: 58.1130\n",
            "Epoch 688/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 25.8107 - val_loss: 58.3690\n",
            "Epoch 689/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 26.9040 - val_loss: 58.7519\n",
            "Epoch 690/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 25.9106 - val_loss: 58.9649\n",
            "Epoch 691/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 26.5893 - val_loss: 59.2050\n",
            "Epoch 692/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 25.8061 - val_loss: 59.3419\n",
            "Epoch 693/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 25.2707 - val_loss: 59.3107\n",
            "Epoch 694/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 28.0455 - val_loss: 59.3516\n",
            "Epoch 695/1000\n",
            "271/271 [==============================] - 0s 519us/step - loss: 24.4708 - val_loss: 59.5079\n",
            "Epoch 696/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 26.1943 - val_loss: 59.4946\n",
            "Epoch 697/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 24.3344 - val_loss: 59.3411\n",
            "Epoch 698/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 26.4018 - val_loss: 59.1384\n",
            "Epoch 699/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 26.3030 - val_loss: 58.8904\n",
            "Epoch 700/1000\n",
            "271/271 [==============================] - 0s 509us/step - loss: 24.6398 - val_loss: 58.4992\n",
            "Epoch 701/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 27.5821 - val_loss: 58.0392\n",
            "Epoch 702/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 23.2208 - val_loss: 57.6796\n",
            "Epoch 703/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 24.8395 - val_loss: 57.4923\n",
            "Epoch 704/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 25.8694 - val_loss: 57.3571\n",
            "Epoch 705/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 24.5307 - val_loss: 57.1289\n",
            "Epoch 706/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 24.7386 - val_loss: 57.0234\n",
            "Epoch 707/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 24.8818 - val_loss: 57.1250\n",
            "Epoch 708/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 23.5663 - val_loss: 57.2723\n",
            "Epoch 709/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 24.6873 - val_loss: 57.5870\n",
            "Epoch 710/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 25.2676 - val_loss: 57.7627\n",
            "Epoch 711/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 25.8222 - val_loss: 57.8349\n",
            "Epoch 712/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 25.4915 - val_loss: 57.8540\n",
            "Epoch 713/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 24.8926 - val_loss: 57.9860\n",
            "Epoch 714/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 25.9523 - val_loss: 58.0539\n",
            "Epoch 715/1000\n",
            "271/271 [==============================] - 0s 514us/step - loss: 24.3158 - val_loss: 58.0319\n",
            "Epoch 716/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 22.9977 - val_loss: 58.1227\n",
            "Epoch 717/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 24.3245 - val_loss: 58.3262\n",
            "Epoch 718/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 24.5198 - val_loss: 58.5220\n",
            "Epoch 719/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 26.2839 - val_loss: 58.7904\n",
            "Epoch 720/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 23.5285 - val_loss: 59.0889\n",
            "Epoch 721/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 24.8059 - val_loss: 59.3604\n",
            "Epoch 722/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 23.0605 - val_loss: 59.6315\n",
            "Epoch 723/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 23.6936 - val_loss: 59.8389\n",
            "Epoch 724/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 24.4597 - val_loss: 60.0351\n",
            "Epoch 725/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 23.8924 - val_loss: 60.1715\n",
            "Epoch 726/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 24.9445 - val_loss: 60.2242\n",
            "Epoch 727/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 23.7454 - val_loss: 60.1753\n",
            "Epoch 728/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 24.6010 - val_loss: 60.0210\n",
            "Epoch 729/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 24.9822 - val_loss: 59.7099\n",
            "Epoch 730/1000\n",
            "271/271 [==============================] - 0s 510us/step - loss: 22.8717 - val_loss: 59.1741\n",
            "Epoch 731/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 23.2028 - val_loss: 58.5929\n",
            "Epoch 732/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 24.9533 - val_loss: 58.0202\n",
            "Epoch 733/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 26.6445 - val_loss: 57.5836\n",
            "Epoch 734/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 22.8393 - val_loss: 57.4665\n",
            "Epoch 735/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 24.7506 - val_loss: 57.5274\n",
            "Epoch 736/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 23.9894 - val_loss: 57.8052\n",
            "Epoch 737/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 23.7141 - val_loss: 58.1904\n",
            "Epoch 738/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 23.8522 - val_loss: 58.5535\n",
            "Epoch 739/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 23.4588 - val_loss: 58.6410\n",
            "Epoch 740/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 23.9079 - val_loss: 58.7109\n",
            "Epoch 741/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 24.3740 - val_loss: 58.6702\n",
            "Epoch 742/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 24.8317 - val_loss: 58.6197\n",
            "Epoch 743/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 22.7060 - val_loss: 58.6667\n",
            "Epoch 744/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 24.5432 - val_loss: 58.6701\n",
            "Epoch 745/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 23.6990 - val_loss: 58.6002\n",
            "Epoch 746/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 22.7816 - val_loss: 58.3617\n",
            "Epoch 747/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 23.8571 - val_loss: 58.0027\n",
            "Epoch 748/1000\n",
            "271/271 [==============================] - 0s 510us/step - loss: 24.1075 - val_loss: 57.6571\n",
            "Epoch 749/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 22.5512 - val_loss: 57.3996\n",
            "Epoch 750/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 23.9188 - val_loss: 57.2712\n",
            "Epoch 751/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 22.6469 - val_loss: 57.2515\n",
            "Epoch 752/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 25.3519 - val_loss: 57.2520\n",
            "Epoch 753/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 25.4668 - val_loss: 57.3207\n",
            "Epoch 754/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 24.3022 - val_loss: 57.3556\n",
            "Epoch 755/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 23.4734 - val_loss: 57.3462\n",
            "Epoch 756/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 23.6383 - val_loss: 57.3582\n",
            "Epoch 757/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 23.7085 - val_loss: 57.4805\n",
            "Epoch 758/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 24.0611 - val_loss: 57.4829\n",
            "Epoch 759/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 23.4925 - val_loss: 57.4288\n",
            "Epoch 760/1000\n",
            "271/271 [==============================] - 0s 511us/step - loss: 23.1780 - val_loss: 57.3491\n",
            "Epoch 761/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 24.0226 - val_loss: 57.2717\n",
            "Epoch 762/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 24.7943 - val_loss: 57.2980\n",
            "Epoch 763/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 23.8428 - val_loss: 57.3418\n",
            "Epoch 764/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 23.2844 - val_loss: 57.4175\n",
            "Epoch 765/1000\n",
            "271/271 [==============================] - 0s 508us/step - loss: 22.8325 - val_loss: 57.4722\n",
            "Epoch 766/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 23.1282 - val_loss: 57.4821\n",
            "Epoch 767/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 22.3545 - val_loss: 57.6021\n",
            "Epoch 768/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 21.5368 - val_loss: 57.6489\n",
            "Epoch 769/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 23.9777 - val_loss: 57.7001\n",
            "Epoch 770/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 23.5309 - val_loss: 57.7463\n",
            "Epoch 771/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 22.1829 - val_loss: 57.7933\n",
            "Epoch 772/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 22.3852 - val_loss: 57.7785\n",
            "Epoch 773/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 23.0264 - val_loss: 57.7602\n",
            "Epoch 774/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 22.5173 - val_loss: 57.7249\n",
            "Epoch 775/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 24.2198 - val_loss: 57.6911\n",
            "Epoch 776/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 22.0960 - val_loss: 57.6847\n",
            "Epoch 777/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 23.1227 - val_loss: 57.7200\n",
            "Epoch 778/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 24.0347 - val_loss: 57.6194\n",
            "Epoch 779/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 23.6565 - val_loss: 57.4655\n",
            "Epoch 780/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 21.6164 - val_loss: 57.2700\n",
            "Epoch 781/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 21.6068 - val_loss: 57.0876\n",
            "Epoch 782/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 23.5593 - val_loss: 56.7671\n",
            "Epoch 783/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 22.6445 - val_loss: 56.4425\n",
            "Epoch 784/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 23.8476 - val_loss: 56.2118\n",
            "Epoch 785/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 22.0392 - val_loss: 56.0462\n",
            "Epoch 786/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 23.1798 - val_loss: 55.9428\n",
            "Epoch 787/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 21.6687 - val_loss: 55.8824\n",
            "Epoch 788/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 22.7587 - val_loss: 55.9577\n",
            "Epoch 789/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 22.7479 - val_loss: 56.0942\n",
            "Epoch 790/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 22.6077 - val_loss: 56.2256\n",
            "Epoch 791/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 22.2042 - val_loss: 56.2261\n",
            "Epoch 792/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 22.3281 - val_loss: 56.2456\n",
            "Epoch 793/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 25.2159 - val_loss: 56.2049\n",
            "Epoch 794/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 23.5385 - val_loss: 56.2003\n",
            "Epoch 795/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 21.0775 - val_loss: 56.1570\n",
            "Epoch 796/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 22.1405 - val_loss: 56.1771\n",
            "Epoch 797/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 23.0161 - val_loss: 56.3300\n",
            "Epoch 798/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 22.0658 - val_loss: 56.4472\n",
            "Epoch 799/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 22.3322 - val_loss: 56.3395\n",
            "Epoch 800/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 20.0438 - val_loss: 56.2110\n",
            "Epoch 801/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 23.9494 - val_loss: 56.1598\n",
            "Epoch 802/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 23.8725 - val_loss: 55.9714\n",
            "Epoch 803/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 23.9881 - val_loss: 55.8378\n",
            "Epoch 804/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 22.4445 - val_loss: 55.8070\n",
            "Epoch 805/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 22.4184 - val_loss: 55.7364\n",
            "Epoch 806/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 22.6422 - val_loss: 55.7542\n",
            "Epoch 807/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 22.1860 - val_loss: 55.7687\n",
            "Epoch 808/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 22.8834 - val_loss: 55.7813\n",
            "Epoch 809/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 22.2807 - val_loss: 55.8459\n",
            "Epoch 810/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 21.9413 - val_loss: 55.7568\n",
            "Epoch 811/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 22.6368 - val_loss: 55.6515\n",
            "Epoch 812/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 21.4307 - val_loss: 55.5272\n",
            "Epoch 813/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 22.3421 - val_loss: 55.2039\n",
            "Epoch 814/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 20.8499 - val_loss: 55.0510\n",
            "Epoch 815/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 21.7442 - val_loss: 54.9793\n",
            "Epoch 816/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 20.7197 - val_loss: 55.0652\n",
            "Epoch 817/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 22.7325 - val_loss: 55.2115\n",
            "Epoch 818/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 21.4677 - val_loss: 55.3735\n",
            "Epoch 819/1000\n",
            "271/271 [==============================] - 0s 523us/step - loss: 22.5096 - val_loss: 55.4782\n",
            "Epoch 820/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 21.9639 - val_loss: 55.3835\n",
            "Epoch 821/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 21.8502 - val_loss: 55.2426\n",
            "Epoch 822/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 21.6867 - val_loss: 55.3384\n",
            "Epoch 823/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 23.0136 - val_loss: 55.3650\n",
            "Epoch 824/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 22.5714 - val_loss: 55.5440\n",
            "Epoch 825/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 20.7759 - val_loss: 55.6418\n",
            "Epoch 826/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 21.6777 - val_loss: 56.0217\n",
            "Epoch 827/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 23.5191 - val_loss: 56.5345\n",
            "Epoch 828/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 20.6510 - val_loss: 56.9762\n",
            "Epoch 829/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 21.3767 - val_loss: 57.2210\n",
            "Epoch 830/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 22.4537 - val_loss: 57.0965\n",
            "Epoch 831/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 19.8700 - val_loss: 56.6303\n",
            "Epoch 832/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 21.7326 - val_loss: 56.2880\n",
            "Epoch 833/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 22.7549 - val_loss: 56.0159\n",
            "Epoch 834/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 23.1424 - val_loss: 56.0301\n",
            "Epoch 835/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 19.5451 - val_loss: 56.3650\n",
            "Epoch 836/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 21.1881 - val_loss: 56.8168\n",
            "Epoch 837/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 20.7787 - val_loss: 57.0923\n",
            "Epoch 838/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 21.4771 - val_loss: 57.0794\n",
            "Epoch 839/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 20.9350 - val_loss: 56.6116\n",
            "Epoch 840/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 21.7510 - val_loss: 56.0798\n",
            "Epoch 841/1000\n",
            "271/271 [==============================] - 0s 480us/step - loss: 20.7344 - val_loss: 55.4983\n",
            "Epoch 842/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 20.0239 - val_loss: 55.0835\n",
            "Epoch 843/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 22.7434 - val_loss: 54.9638\n",
            "Epoch 844/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 22.3122 - val_loss: 54.9945\n",
            "Epoch 845/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 21.1010 - val_loss: 55.1454\n",
            "Epoch 846/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 21.4156 - val_loss: 55.4552\n",
            "Epoch 847/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 21.5231 - val_loss: 55.8329\n",
            "Epoch 848/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 20.9566 - val_loss: 55.9729\n",
            "Epoch 849/1000\n",
            "271/271 [==============================] - 0s 505us/step - loss: 21.9515 - val_loss: 55.8213\n",
            "Epoch 850/1000\n",
            "271/271 [==============================] - 0s 510us/step - loss: 23.0061 - val_loss: 55.6269\n",
            "Epoch 851/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 20.9035 - val_loss: 55.2848\n",
            "Epoch 852/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 19.9129 - val_loss: 54.9306\n",
            "Epoch 853/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 20.0065 - val_loss: 54.7359\n",
            "Epoch 854/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 20.7826 - val_loss: 54.6150\n",
            "Epoch 855/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 21.2981 - val_loss: 54.7399\n",
            "Epoch 856/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 19.8335 - val_loss: 55.1959\n",
            "Epoch 857/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 20.5654 - val_loss: 55.7171\n",
            "Epoch 858/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 20.2674 - val_loss: 56.2121\n",
            "Epoch 859/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 22.6732 - val_loss: 56.5353\n",
            "Epoch 860/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 22.4471 - val_loss: 56.4692\n",
            "Epoch 861/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 22.6669 - val_loss: 56.2225\n",
            "Epoch 862/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 20.9266 - val_loss: 55.8973\n",
            "Epoch 863/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 22.6733 - val_loss: 55.3693\n",
            "Epoch 864/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 20.0800 - val_loss: 54.9336\n",
            "Epoch 865/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 20.0676 - val_loss: 54.7961\n",
            "Epoch 866/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 19.5890 - val_loss: 54.7564\n",
            "Epoch 867/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 20.7947 - val_loss: 54.8633\n",
            "Epoch 868/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 20.9223 - val_loss: 55.0110\n",
            "Epoch 869/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 19.6321 - val_loss: 54.9517\n",
            "Epoch 870/1000\n",
            "271/271 [==============================] - 0s 503us/step - loss: 19.0402 - val_loss: 54.8517\n",
            "Epoch 871/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 21.5413 - val_loss: 54.6420\n",
            "Epoch 872/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 21.9068 - val_loss: 54.3287\n",
            "Epoch 873/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 21.6787 - val_loss: 53.9270\n",
            "Epoch 874/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 20.0725 - val_loss: 53.5749\n",
            "Epoch 875/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 20.6654 - val_loss: 53.2375\n",
            "Epoch 876/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 19.1093 - val_loss: 53.0544\n",
            "Epoch 877/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 20.5204 - val_loss: 52.9417\n",
            "Epoch 878/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 22.0064 - val_loss: 52.8471\n",
            "Epoch 879/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 19.8556 - val_loss: 52.7936\n",
            "Epoch 880/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 20.5160 - val_loss: 52.7821\n",
            "Epoch 881/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 23.4768 - val_loss: 52.6935\n",
            "Epoch 882/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 21.7382 - val_loss: 52.5278\n",
            "Epoch 883/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 21.3731 - val_loss: 52.2593\n",
            "Epoch 884/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 20.5590 - val_loss: 52.0842\n",
            "Epoch 885/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 20.8231 - val_loss: 52.0068\n",
            "Epoch 886/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 19.5695 - val_loss: 52.1171\n",
            "Epoch 887/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 20.1495 - val_loss: 52.1511\n",
            "Epoch 888/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 19.3250 - val_loss: 52.2955\n",
            "Epoch 889/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 21.4477 - val_loss: 52.2963\n",
            "Epoch 890/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 20.4039 - val_loss: 52.1943\n",
            "Epoch 891/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 20.9471 - val_loss: 52.1180\n",
            "Epoch 892/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 18.8787 - val_loss: 52.0712\n",
            "Epoch 893/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 19.6751 - val_loss: 51.9447\n",
            "Epoch 894/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.7084 - val_loss: 51.9051\n",
            "Epoch 895/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 19.2618 - val_loss: 51.8643\n",
            "Epoch 896/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 19.1487 - val_loss: 52.0491\n",
            "Epoch 897/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 19.0906 - val_loss: 52.2763\n",
            "Epoch 898/1000\n",
            "271/271 [==============================] - 0s 507us/step - loss: 20.1037 - val_loss: 52.3241\n",
            "Epoch 899/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 19.5148 - val_loss: 52.3602\n",
            "Epoch 900/1000\n",
            "271/271 [==============================] - 0s 500us/step - loss: 19.3216 - val_loss: 52.2078\n",
            "Epoch 901/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 20.8751 - val_loss: 52.0411\n",
            "Epoch 902/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 20.4052 - val_loss: 51.8853\n",
            "Epoch 903/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.0480 - val_loss: 51.6846\n",
            "Epoch 904/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 19.8175 - val_loss: 51.6068\n",
            "Epoch 905/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.0784 - val_loss: 51.8322\n",
            "Epoch 906/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 17.8362 - val_loss: 52.4594\n",
            "Epoch 907/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.2828 - val_loss: 52.7936\n",
            "Epoch 908/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 19.3959 - val_loss: 52.9009\n",
            "Epoch 909/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 19.6640 - val_loss: 52.9078\n",
            "Epoch 910/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 19.0607 - val_loss: 52.5994\n",
            "Epoch 911/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 19.0308 - val_loss: 52.5086\n",
            "Epoch 912/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 17.8293 - val_loss: 52.5509\n",
            "Epoch 913/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 19.4684 - val_loss: 52.7315\n",
            "Epoch 914/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.3918 - val_loss: 53.0284\n",
            "Epoch 915/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.6093 - val_loss: 53.3520\n",
            "Epoch 916/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 19.0978 - val_loss: 53.3537\n",
            "Epoch 917/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 18.6905 - val_loss: 52.7458\n",
            "Epoch 918/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 19.0781 - val_loss: 51.9234\n",
            "Epoch 919/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 18.6126 - val_loss: 51.3464\n",
            "Epoch 920/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 17.7048 - val_loss: 50.8915\n",
            "Epoch 921/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 20.7351 - val_loss: 50.6293\n",
            "Epoch 922/1000\n",
            "271/271 [==============================] - 0s 515us/step - loss: 18.4282 - val_loss: 50.4670\n",
            "Epoch 923/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 20.2312 - val_loss: 50.4027\n",
            "Epoch 924/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 20.4379 - val_loss: 50.3518\n",
            "Epoch 925/1000\n",
            "271/271 [==============================] - 0s 483us/step - loss: 19.6888 - val_loss: 50.3653\n",
            "Epoch 926/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 18.0447 - val_loss: 50.3769\n",
            "Epoch 927/1000\n",
            "271/271 [==============================] - 0s 509us/step - loss: 18.3488 - val_loss: 50.3419\n",
            "Epoch 928/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 20.2810 - val_loss: 50.1176\n",
            "Epoch 929/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 19.3277 - val_loss: 49.7455\n",
            "Epoch 930/1000\n",
            "271/271 [==============================] - 0s 489us/step - loss: 18.9233 - val_loss: 49.3247\n",
            "Epoch 931/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 18.1658 - val_loss: 48.9232\n",
            "Epoch 932/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 18.4638 - val_loss: 48.6973\n",
            "Epoch 933/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 18.4823 - val_loss: 48.7256\n",
            "Epoch 934/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 17.2408 - val_loss: 48.8536\n",
            "Epoch 935/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.2312 - val_loss: 49.0239\n",
            "Epoch 936/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 18.0525 - val_loss: 49.0926\n",
            "Epoch 937/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 18.2765 - val_loss: 48.8352\n",
            "Epoch 938/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 19.8429 - val_loss: 48.9660\n",
            "Epoch 939/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 18.6522 - val_loss: 48.7833\n",
            "Epoch 940/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 18.5695 - val_loss: 48.5984\n",
            "Epoch 941/1000\n",
            "271/271 [==============================] - 0s 504us/step - loss: 19.5260 - val_loss: 48.3860\n",
            "Epoch 942/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 20.2509 - val_loss: 48.1303\n",
            "Epoch 943/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 19.2316 - val_loss: 47.9173\n",
            "Epoch 944/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 17.9672 - val_loss: 47.8465\n",
            "Epoch 945/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.5133 - val_loss: 47.7335\n",
            "Epoch 946/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 19.2583 - val_loss: 47.6831\n",
            "Epoch 947/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 17.8773 - val_loss: 47.6761\n",
            "Epoch 948/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 18.2122 - val_loss: 47.6795\n",
            "Epoch 949/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 18.2034 - val_loss: 47.7057\n",
            "Epoch 950/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 18.7919 - val_loss: 47.6883\n",
            "Epoch 951/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 17.4351 - val_loss: 47.5713\n",
            "Epoch 952/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 17.5045 - val_loss: 47.4790\n",
            "Epoch 953/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 17.9787 - val_loss: 47.4287\n",
            "Epoch 954/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 15.9296 - val_loss: 47.3447\n",
            "Epoch 955/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 18.1074 - val_loss: 47.3274\n",
            "Epoch 956/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 19.2828 - val_loss: 47.2515\n",
            "Epoch 957/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 18.8549 - val_loss: 47.1485\n",
            "Epoch 958/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 18.0982 - val_loss: 47.0401\n",
            "Epoch 959/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.8062 - val_loss: 46.8167\n",
            "Epoch 960/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 17.6268 - val_loss: 46.7688\n",
            "Epoch 961/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 18.9701 - val_loss: 46.8017\n",
            "Epoch 962/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 17.4625 - val_loss: 46.7864\n",
            "Epoch 963/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 17.8148 - val_loss: 46.7830\n",
            "Epoch 964/1000\n",
            "271/271 [==============================] - 0s 492us/step - loss: 17.4366 - val_loss: 46.8630\n",
            "Epoch 965/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 17.1109 - val_loss: 46.7109\n",
            "Epoch 966/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 16.8102 - val_loss: 46.3913\n",
            "Epoch 967/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 17.2243 - val_loss: 45.9688\n",
            "Epoch 968/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 17.6129 - val_loss: 45.6628\n",
            "Epoch 969/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 19.3302 - val_loss: 45.6294\n",
            "Epoch 970/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 18.1102 - val_loss: 45.6832\n",
            "Epoch 971/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 18.5980 - val_loss: 45.9456\n",
            "Epoch 972/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 18.0530 - val_loss: 46.1836\n",
            "Epoch 973/1000\n",
            "271/271 [==============================] - 0s 501us/step - loss: 18.8140 - val_loss: 46.2958\n",
            "Epoch 974/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 17.0660 - val_loss: 46.2071\n",
            "Epoch 975/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 17.1856 - val_loss: 46.1219\n",
            "Epoch 976/1000\n",
            "271/271 [==============================] - 0s 494us/step - loss: 16.6120 - val_loss: 46.0713\n",
            "Epoch 977/1000\n",
            "271/271 [==============================] - 0s 496us/step - loss: 18.5602 - val_loss: 46.0115\n",
            "Epoch 978/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 17.5771 - val_loss: 45.9006\n",
            "Epoch 979/1000\n",
            "271/271 [==============================] - 0s 484us/step - loss: 16.9809 - val_loss: 45.7685\n",
            "Epoch 980/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 17.2054 - val_loss: 45.7478\n",
            "Epoch 981/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 16.7384 - val_loss: 45.7314\n",
            "Epoch 982/1000\n",
            "271/271 [==============================] - 0s 485us/step - loss: 16.8757 - val_loss: 45.6821\n",
            "Epoch 983/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 17.2534 - val_loss: 45.6612\n",
            "Epoch 984/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 17.8824 - val_loss: 45.5601\n",
            "Epoch 985/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 18.2042 - val_loss: 45.3200\n",
            "Epoch 986/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.5963 - val_loss: 45.2984\n",
            "Epoch 987/1000\n",
            "271/271 [==============================] - 0s 510us/step - loss: 17.8204 - val_loss: 45.4297\n",
            "Epoch 988/1000\n",
            "271/271 [==============================] - 0s 498us/step - loss: 18.3240 - val_loss: 45.6016\n",
            "Epoch 989/1000\n",
            "271/271 [==============================] - 0s 488us/step - loss: 18.5761 - val_loss: 45.7808\n",
            "Epoch 990/1000\n",
            "271/271 [==============================] - 0s 491us/step - loss: 17.6477 - val_loss: 46.0277\n",
            "Epoch 991/1000\n",
            "271/271 [==============================] - 0s 487us/step - loss: 18.0669 - val_loss: 46.3017\n",
            "Epoch 992/1000\n",
            "271/271 [==============================] - 0s 508us/step - loss: 16.7721 - val_loss: 46.2259\n",
            "Epoch 993/1000\n",
            "271/271 [==============================] - 0s 486us/step - loss: 16.6093 - val_loss: 46.1493\n",
            "Epoch 994/1000\n",
            "271/271 [==============================] - 0s 495us/step - loss: 17.9844 - val_loss: 46.3594\n",
            "Epoch 995/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 16.1391 - val_loss: 46.5874\n",
            "Epoch 996/1000\n",
            "271/271 [==============================] - 0s 497us/step - loss: 16.8324 - val_loss: 46.6257\n",
            "Epoch 997/1000\n",
            "271/271 [==============================] - 0s 499us/step - loss: 18.3478 - val_loss: 46.4518\n",
            "Epoch 998/1000\n",
            "271/271 [==============================] - 0s 493us/step - loss: 16.6659 - val_loss: 46.2347\n",
            "Epoch 999/1000\n",
            "271/271 [==============================] - 0s 490us/step - loss: 18.4072 - val_loss: 45.9225\n",
            "Epoch 1000/1000\n",
            "271/271 [==============================] - 0s 502us/step - loss: 16.3307 - val_loss: 45.7200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8b7feabf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7mTbTBatYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b3e90700-4876-47e1-fd00-4b45560b55ba"
      },
      "source": [
        "preds = model.predict([testAttrX, testImagesX])\n",
        "\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 135.77%, std: 59.25%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}